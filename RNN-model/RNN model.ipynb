{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d47912",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using - RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8e4191",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f741b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to JSON dataset\n",
    "file_path = r'C:\\Users\\Administrator\\Documents\\GitHub\\Datasets\\yelp_academic_dataset_review.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0166fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in chunks\n",
    "chunk_size = 1000000\n",
    "model_save_path = 'sentiment_analysis_rnn_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0fb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model outside the loop\n",
    "model = None\n",
    "epochs = 1\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1ccf9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     review_id                 user_id  \\\n",
      "0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
      "1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
      "2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n",
      "3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
      "4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n",
      "...                        ...                     ...   \n",
      "999995  t-2o35kr7Q9DSaeuKhaDuQ  oX7o1TH0PHUWp9r9ry9_vw   \n",
      "999996  fLIwWCvdul9PNWYfJt5QWA  v8wlapFKVLs2qTYCGhCdiw   \n",
      "999997  ETAiy6wEM-r9ve4SKDhBpg  rLlYc1RzIBnOmnX3AbpEYw   \n",
      "999998  8OgvSXuc6KjAt2fSz9LuzA  eEH-8CEPU5ndPxDGzVfHiQ   \n",
      "999999  TQoVzNXqDkiDXgzlw8cyaQ  HYmGwYXvcYmW7dDjuWKJfw   \n",
      "\n",
      "                   business_id  stars  useful  funny  cool  \\\n",
      "0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n",
      "1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n",
      "2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n",
      "3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n",
      "4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n",
      "...                        ...    ...     ...    ...   ...   \n",
      "999995  jLn69WQupjsDKrbPw_nlGQ      3       0      1     0   \n",
      "999996  t6v8g8UeNiq3O2GoEc7R4Q      4       0      0     0   \n",
      "999997  ZYRul0i1bhOjirHED6Kd0w      3       0      0     0   \n",
      "999998  onGXKwnxPLtKnO8yqQMPSA      1       1      0     1   \n",
      "999999  54vEFcAri3Wj6cdM4ljDBA      4       2      1     0   \n",
      "\n",
      "                                                     text                date  \n",
      "0       If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1       I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2       Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3       Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4       Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
      "...                                                   ...                 ...  \n",
      "999995  Never really had any issues here other than th... 2017-11-15 09:43:07  \n",
      "999996  Fish recently moved a couple of doors down the... 2014-09-03 18:27:33  \n",
      "999997  I've been to South House around a dozen or so ... 2016-02-20 22:25:29  \n",
      "999998  Wow! I am shocked at these reviews. I have tri... 2010-06-27 02:17:30  \n",
      "999999  headed to St. Louis for a little getaway with ... 2012-11-05 03:25:02  \n",
      "\n",
      "[1000000 rows x 9 columns]\n",
      "5625/5625 [==============================] - 144s 25ms/step - loss: 0.3961 - accuracy: 0.8499 - val_loss: 0.3402 - val_accuracy: 0.8691\n",
      "Epoch 1 - Loss: 0.39606422185897827, Accuracy: 0.8499388694763184, Validation Loss: 0.34020307660102844, Validation Accuracy: 0.8691250085830688\n",
      "                      review_id                 user_id  \\\n",
      "1000000  1yzkzMBuDc0oIATmm-YT6A  batsML880B8-BMV1YBh0VQ   \n",
      "1000001  p1He-R5tvsxamxNF0Bf-cg  T9n6TKhnkqwh7u3a70O-pQ   \n",
      "1000002  fOOZEC8D2f1hI6bs08X3gQ  4zUi7exGanxHz_zXJuHjMw   \n",
      "1000003  ZoeZ_aC_CiWSuqvfe38nAQ  bHNxtcIKeU5Ifnd4BwBoWw   \n",
      "1000004  ygAtxZ1Gaimqj8IZGPobRA  W9zN7u6b-e3bkZ69W92fig   \n",
      "...                         ...                     ...   \n",
      "1999995  5Kj5gEHBP8SIYeiwaNV9xg  Ux0CsHDQvEbtZXlGDgcCHg   \n",
      "1999996  wQq7vGWf8euzUk7TkoVAXw  5jKScvMqxW_rzd0D8xgX1A   \n",
      "1999997  eAKeO7EX8kSKsCBcq7mKTA  _Redmmq2oiUvMFhzS-CAVA   \n",
      "1999998  HaY4k2iVPhcTOwfGVPlYSg  BnMfYA-c2YQbzVMto2wO5w   \n",
      "1999999  MmFruENWfbqghG5hwJhIaw  p465OiphoWkYQDnbK_hLIg   \n",
      "\n",
      "                    business_id  stars  useful  funny  cool  \\\n",
      "1000000  NLV0ppsHTiJk6JVdFSxVAQ      5       1      0     0   \n",
      "1000001  c_xAbBTeiIHqPm1zMOvQcg      5       0      0     0   \n",
      "1000002  Sv1MEZP-mMfp8SmE0hwYEA      5       0      0     0   \n",
      "1000003  KBG3GV4WHU-iLLSWiapbJw      4       0      0     0   \n",
      "1000004  BX3LLonrLLLfVX-tNrE7yA      5       1      0     0   \n",
      "...                         ...    ...     ...    ...   ...   \n",
      "1999995  TGiTwNYy3ZvOoHTT7ZHpgg      1       0      0     0   \n",
      "1999996  kOmhn5Eo6AWzMVL6ALRq2Q      5       2      0     1   \n",
      "1999997  m6ZcOrHJHYmIPhjgh8cycw      2       0      0     0   \n",
      "1999998  YCfMNP_1YCTXTKlrnw44vQ      5       0      0     0   \n",
      "1999999  YceIYPMBP3oKcuEFhj2_Gg      5       0      0     0   \n",
      "\n",
      "                                                      text                date  \n",
      "1000000  Pleasantly intrigued by the combination of bre... 2016-05-29 00:29:56  \n",
      "1000001  I am not sure what I was expecting, but this w... 2016-11-19 04:50:47  \n",
      "1000002  Mmm....still thinking about it right now. The ... 2015-01-18 06:41:40  \n",
      "1000003  Very good even by Nashville and non-Nashville ... 2011-05-10 02:24:14  \n",
      "1000004  New location is beautiful! This location is st... 2017-11-28 18:22:51  \n",
      "...                                                    ...                 ...  \n",
      "1999995  Had to wait over 20 minutes for my order, then... 2020-09-20 00:13:27  \n",
      "1999996  Very good food!! My wife and I got crab legs, ... 2020-06-11 23:25:17  \n",
      "1999997  Food was just ok, nothing to write home about.... 2020-12-02 23:02:55  \n",
      "1999998  Great spot!   Food and service was terrific!  ... 2021-01-09 02:45:57  \n",
      "1999999  My ten-year-old Sheltie, Diane, had a severe l... 2020-12-04 22:23:55  \n",
      "\n",
      "[1000000 rows x 9 columns]\n",
      "5625/5625 [==============================] - 146s 26ms/step - loss: 0.3835 - accuracy: 0.8552 - val_loss: 0.3249 - val_accuracy: 0.8765\n",
      "Epoch 1 - Loss: 0.3834611475467682, Accuracy: 0.8551527857780457, Validation Loss: 0.3249252438545227, Validation Accuracy: 0.8765375018119812\n",
      "                      review_id                 user_id  \\\n",
      "2000000  NxIV6YoOfsiBSxrNF5i1Dw  r5YAim_lxd4Js2gQVGeLsw   \n",
      "2000001  S4ULMYax-92NERQvAivrfw  QJIbIcjPb6qhRnb1d4sTjg   \n",
      "2000002  uClUShLNMwHJNLgS9J5poQ  IiVjDaSDELbxLIWu0_Ucrw   \n",
      "2000003  3DrBtGCx4N3aXrbpDAxExg  K2vRxDY1uuxtm_YEEfQ9wQ   \n",
      "2000004  uNXSrvw9jO4jarViJcfmvg  ctgPXpW4NitQisBWHaS1YA   \n",
      "...                         ...                     ...   \n",
      "2999995  gZlc76DGyo3IK4tznEdO9A  yy0MrguMeuPuojEZdlEVHA   \n",
      "2999996  Ju60Eqh9BwzkokqW3orChg  mjxWgGxFIIy13Vo02_HksQ   \n",
      "2999997  b6NXauz7dbzWXf6Y8zNbVA  WWfAP9hq9ldiUOm5ijQliw   \n",
      "2999998  E8G36-PTiuH4U4j8yKSNQw  eLkEcFGpwqMaL6Cq5axmcA   \n",
      "2999999  yAUrbmFOYwg9CgeKUPvSZA  Sejmtp5di6lxUcEWJV8TnQ   \n",
      "\n",
      "                    business_id  stars  useful  funny  cool  \\\n",
      "2000000  T3Bbtht3jO2izyO6f6W8bg      5       3      0     4   \n",
      "2000001  EY5cTfLf9BNG3tgoez9fKg      5       0      0     0   \n",
      "2000002  6nCLoE5WfY69AywtTmg2Lg      5       2      0     0   \n",
      "2000003  wMQMMxaGq0HPG0mApezXMw      5       1      0     0   \n",
      "2000004  5uhG34VRsD0A6lOY8F41Dg      5       0      0     1   \n",
      "...                         ...    ...     ...    ...   ...   \n",
      "2999995  2TW0TCqrRxyolGb3S_uTiA      3       0      0     0   \n",
      "2999996  gEmVqHPesM4z0dC3nOnkUw      4       3      8     1   \n",
      "2999997  HotEtVKhb7SVb7AFgp4O1g      5       0      0     0   \n",
      "2999998  6NFKMheqAVk9Sa2hqssAMw      5       1      0     0   \n",
      "2999999  Ydu9tZPmGV31A2wgsLhyPA      5       0      0     0   \n",
      "\n",
      "                                                      text                date  \n",
      "2000000  Pros: Makaylah: our bartender and server, she ... 2020-10-31 00:18:49  \n",
      "2000001  Authentic and Perfect! \\nAs someone who's as b... 2021-02-09 03:12:34  \n",
      "2000002  Great fresh food for lunch, I had the appetize... 2020-10-30 00:04:00  \n",
      "2000003  I heard good things about Ahi when I moved to ... 2020-05-03 01:21:43  \n",
      "2000004  The Habit is yummy. Had a charburger with chee... 2021-02-09 05:32:37  \n",
      "...                                                    ...                 ...  \n",
      "2999995  This place is quite meh. It's nothing phenomen... 2016-08-18 23:24:18  \n",
      "2999996  Nah for real... I'm bouta put y'all on some re... 2016-04-06 20:16:10  \n",
      "2999997  The chicken curry here is the best I've had in... 2013-09-17 15:57:33  \n",
      "2999998  It is really hard to compete with Red Robin bu... 2018-06-30 02:21:51  \n",
      "2999999  Loved this place!! The masala dosas are out of... 2016-09-11 16:20:41  \n",
      "\n",
      "[1000000 rows x 9 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10884\\2904095172.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Tokenize the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<OOV>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                 \u001b[1;31m# In how many documents each word occurs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mwcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the data in chunks\n",
    "for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "    print(chunk)\n",
    "\n",
    "    # Accessing the data DataFrame\n",
    "    texts = chunk['text'].tolist()\n",
    "    labels = pd.cut(chunk['stars'], bins=[0, 2, 3, 5], labels=[1, 2, 3])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Padding sequences\n",
    "    max_len = 100\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Convert labels to numerical format\n",
    "    numeric_labels = np.array(labels)\n",
    "\n",
    "    # Convert to one-hot encoding for multi-class classification\n",
    "    labels_one_hot = pd.get_dummies(labels).values\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build or load the RNN model\n",
    "    if model is None or not os.path.exists(model_save_path):\n",
    "        embedding_dim = 50\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])  # Categorical crossentropy loss\n",
    "\n",
    "    # Train the model and store the history\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_split=0.1, verbose=1)\n",
    "\n",
    "    # Display the current epoch and metrics\n",
    "    print(f\"Epoch {epochs} - Loss: {history.history['loss'][-1]}, Accuracy: {history.history['accuracy'][-1]}, Validation Loss: {history.history['val_loss'][-1]}, Validation Accuracy: {history.history['val_accuracy'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the final loop's history for plotting\n",
    "final_history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6516a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(final_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(final_history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(final_history['loss'], label='Training Loss')\n",
    "plt.plot(final_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47028e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['1', '2', '3'], yticklabels=['1', '2', '3'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"RNN_model_final.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
