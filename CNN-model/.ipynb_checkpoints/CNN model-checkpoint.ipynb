{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aafb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1003274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to JSON dataset\n",
    "file_path = r'C:\\Users\\Administrator\\Documents\\GitHub\\Datasets\\yelp_academic_dataset_review.json'\n",
    "\n",
    "# Load data in chunks\n",
    "chunk_size = 1000000\n",
    "model_save_path = 'sentiment_analysis_cnn_model.h5'\n",
    "\n",
    "# Initialize the model outside the loop\n",
    "model = None\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     review_id                 user_id  \\\n",
      "0       KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
      "1       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
      "2       saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n",
      "3       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
      "4       Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n",
      "...                        ...                     ...   \n",
      "999995  t-2o35kr7Q9DSaeuKhaDuQ  oX7o1TH0PHUWp9r9ry9_vw   \n",
      "999996  fLIwWCvdul9PNWYfJt5QWA  v8wlapFKVLs2qTYCGhCdiw   \n",
      "999997  ETAiy6wEM-r9ve4SKDhBpg  rLlYc1RzIBnOmnX3AbpEYw   \n",
      "999998  8OgvSXuc6KjAt2fSz9LuzA  eEH-8CEPU5ndPxDGzVfHiQ   \n",
      "999999  TQoVzNXqDkiDXgzlw8cyaQ  HYmGwYXvcYmW7dDjuWKJfw   \n",
      "\n",
      "                   business_id  stars  useful  funny  cool  \\\n",
      "0       XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n",
      "1       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n",
      "2       YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n",
      "3       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n",
      "4       e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n",
      "...                        ...    ...     ...    ...   ...   \n",
      "999995  jLn69WQupjsDKrbPw_nlGQ      3       0      1     0   \n",
      "999996  t6v8g8UeNiq3O2GoEc7R4Q      4       0      0     0   \n",
      "999997  ZYRul0i1bhOjirHED6Kd0w      3       0      0     0   \n",
      "999998  onGXKwnxPLtKnO8yqQMPSA      1       1      0     1   \n",
      "999999  54vEFcAri3Wj6cdM4ljDBA      4       2      1     0   \n",
      "\n",
      "                                                     text                date  \n",
      "0       If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1       I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2       Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3       Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4       Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
      "...                                                   ...                 ...  \n",
      "999995  Never really had any issues here other than th... 2017-11-15 09:43:07  \n",
      "999996  Fish recently moved a couple of doors down the... 2014-09-03 18:27:33  \n",
      "999997  I've been to South House around a dozen or so ... 2016-02-20 22:25:29  \n",
      "999998  Wow! I am shocked at these reviews. I have tri... 2010-06-27 02:17:30  \n",
      "999999  headed to St. Louis for a little getaway with ... 2012-11-05 03:25:02  \n",
      "\n",
      "[1000000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "    print(chunk)\n",
    "    # Accessing the data DataFrame\n",
    "    texts = chunk['text'].tolist()\n",
    "    labels = chunk['stars'].tolist()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Padding sequences\n",
    "    max_len = 100 \n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Convert labels to numerical format\n",
    "    numeric_labels = np.array(labels)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(padded_sequences, numeric_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build or load the CNN model\n",
    "    if model is None or not os.path.exists(model_save_path):\n",
    "        embedding_dim = 50\n",
    "        filters = 64\n",
    "        kernel_size = 3\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len))\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(1, activation='linear'))  # Used linear activation for regression\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')  # Use mean squared error for regression\n",
    "    else:\n",
    "        # Load the previously trained model\n",
    "        model = load_model(model_save_path)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928266ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to an HDF5 file\n",
    "model.save('final_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0853d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
